{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "#os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'MLDatabases' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Andrew123098/MLDatabases.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in 0 Degree .WAV Data.\n",
    "X0 = []\n",
    "count1 = 0\n",
    "database_path = \"MLDatabases/0\"\n",
    "for filename in glob.glob(os.path.join(database_path, '*.wav')):\n",
    "    X0.append(wavfile.read(filename)[1])\n",
    "    count1 = count1 + 1\n",
    "\n",
    "# Load in 60 Degree .WAV Data.\n",
    "X60 = []\n",
    "count1 = 0\n",
    "database_path = \"MLDatabases/60\"\n",
    "for filename in glob.glob(os.path.join(database_path, '*.wav')):\n",
    "    X60.append(wavfile.read(filename)[1])\n",
    "    count1 = count1 + 1\n",
    "\n",
    "# Load in 120 Degree .WAV Data.\n",
    "X120 = []\n",
    "count1 = 0\n",
    "database_path = \"MLDatabases/120\"\n",
    "for filename in glob.glob(os.path.join(database_path, '*.wav')):\n",
    "    X120.append(wavfile.read(filename)[1])\n",
    "    count1 = count1 + 1\n",
    "\n",
    "# Load in 180 Degree .WAV Data.\n",
    "X180 = []\n",
    "count1 = 0\n",
    "database_path = \"MLDatabases/180\"\n",
    "for filename in glob.glob(os.path.join(database_path, '*.wav')):\n",
    "    X180.append(wavfile.read(filename)[1])\n",
    "    count1 = count1 + 1\n",
    "\n",
    "# Load in 240 Degree .WAV Data.\n",
    "X240 = []\n",
    "count1 = 0\n",
    "database_path = \"MLDatabases/240\"\n",
    "for filename in glob.glob(os.path.join(database_path, '*.wav')):\n",
    "    X240.append(wavfile.read(filename)[1])\n",
    "    count1 = count1 + 1\n",
    "\n",
    "# Load in 300 Degree .WAV Data.\n",
    "X300 = []\n",
    "count1 = 0\n",
    "database_path = \"MLDatabases/300\"\n",
    "for filename in glob.glob(os.path.join(database_path, '*.wav')):\n",
    "    X300.append(wavfile.read(filename)[1])\n",
    "    count1 = count1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize and Reformat Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168\n",
      "X0: 154 \n",
      "X300: 154\n"
     ]
    }
   ],
   "source": [
    "# Get the nth smallest size audio file (this will be sample size input to model)\n",
    "n=5\n",
    "X0 = sorted(X0,key=len)\n",
    "X60 = sorted(X60,key=len)\n",
    "X120 = sorted(X120,key=len)\n",
    "X180 = sorted(X180,key=len)\n",
    "X240 = sorted(X240,key=len)\n",
    "X300 = sorted(X300,key=len)\n",
    "X0=X0[n:]\n",
    "X60=X60[n:]\n",
    "X120=X120[n:]\n",
    "X180=X180[n:]\n",
    "X240=X240[n:]\n",
    "X300=X300[n:]\n",
    "sample_size = len(X0[0])\n",
    "print(sample_size)\n",
    "print(\"X0:\",len(X0),\"\\nX300:\",len(X300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Audio Data into Equal Size Chunks and Concatenate into 1 Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (6096, 7168, 2) \n",
      "Y: (6096,)\n"
     ]
    }
   ],
   "source": [
    "# Make audio data into equal size chunks\n",
    "X0e = []\n",
    "for i in X0:\n",
    "    num_chunks = len(i)//sample_size\n",
    "    for j in range(num_chunks):\n",
    "        X0e.append(i[(j+1)*sample_size-sample_size:(j+1)*sample_size])\n",
    "X0 = X0e\n",
    "        \n",
    "X60e = []\n",
    "for i in X60:\n",
    "    num_chunks = len(i)//sample_size\n",
    "    for j in range(num_chunks):\n",
    "        X60e.append(i[(j+1)*sample_size-sample_size:(j+1)*sample_size])        \n",
    "X60=X60e\n",
    "\n",
    "X120e = []\n",
    "for i in X120:\n",
    "    num_chunks = len(i)//sample_size\n",
    "    for j in range(num_chunks):\n",
    "        X120e.append(i[(j+1)*sample_size-sample_size:(j+1)*sample_size])        \n",
    "X120=X120e\n",
    "\n",
    "X180e = []\n",
    "for i in X180:\n",
    "    num_chunks = len(i)//sample_size\n",
    "    for j in range(num_chunks):\n",
    "        X180e.append(i[(j+1)*sample_size-sample_size:(j+1)*sample_size])        \n",
    "X180=X180e\n",
    "\n",
    "X240e = []\n",
    "for i in X240:\n",
    "    num_chunks = len(i)//sample_size\n",
    "    for j in range(num_chunks):\n",
    "        X240e.append(i[(j+1)*sample_size-sample_size:(j+1)*sample_size])        \n",
    "X240=X240e\n",
    "\n",
    "X300e = []\n",
    "for i in X300:\n",
    "    num_chunks = len(i)//sample_size\n",
    "    for j in range(num_chunks):\n",
    "        X300e.append(i[(j+1)*sample_size-sample_size:(j+1)*sample_size])        \n",
    "X300=X300e\n",
    "\n",
    "del X0e\n",
    "del X60e\n",
    "del X120e   \n",
    "del X180e  \n",
    "del X240e  \n",
    "del X300e  \n",
    "\n",
    "# Create Output data that is the same length as the input data.\n",
    "Y0 = np.zeros([X0.__len__()],dtype='float32').tolist()\n",
    "Y60 = (np.ones([X60.__len__()],dtype='float32')).tolist()\n",
    "Y120 = (np.ones([X120.__len__()],dtype='float32')*2).tolist()\n",
    "Y180 = (np.ones([X180.__len__()],dtype='float32')*3).tolist()\n",
    "Y240 = (np.ones([X240.__len__()],dtype='float32')*4).tolist()\n",
    "Y300 = (np.ones([X300.__len__()],dtype='float32')*5).tolist()\n",
    "\n",
    "# Concatenate Left and Right .WAV data and output data as numpy arrays.\n",
    "X0.extend(X60)\n",
    "X0.extend(X120)\n",
    "X0.extend(X180)\n",
    "X0.extend(X240)\n",
    "X0.extend(X300)\n",
    "X = np.asarray(X0)\n",
    "Y = np.asarray(Y0+Y60+Y120+Y180+Y240+Y300).astype(np.int16)\n",
    "print(\"X\",X.shape,\"\\nY:\",Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into Test and Train Data of Sizes Divisible by Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (5080, 7168, 2) \n",
      "Y_train: (5080,) \n",
      "X_test: (1016, 7168, 2) \n",
      "Y_test: (1016,)\n"
     ]
    }
   ],
   "source": [
    "# Make Divisible to Neaest 1000.\n",
    " \n",
    "# Split data into test training data.          ## CHANGE TEST_SIZE HERE ##\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.1666,random_state=0,shuffle=True,stratify=Y)\n",
    "\n",
    "print(\"X_train:\",X_train.shape,\"\\nY_train:\",Y_train.shape,\"\\nX_test:\",X_test.shape,\"\\nY_test:\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_0 (1000, 7168, 2) \n",
      "X_1 (1000, 7168, 2) \n",
      "X_2: (1000, 7168, 2) \n",
      "X_3: (1000, 7168, 2) \n",
      "X_4: (1000, 7168, 2) \n",
      "X_5: (1000, 7168, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split Data into Chunks of 1000\n",
    "X_test = X_test[0:1000][:][:]\n",
    "Y_test = Y_test[0:1000][:][:]\n",
    "X_train = X_train[0:5000][:][:]\n",
    "Y_train = Y_train[0:5000][:][:]\n",
    "\n",
    "X_0 = X_test\n",
    "Y_0 = Y_test\n",
    "X_1 = X_train[0:1000][:][:]\n",
    "Y_1 = Y_train[0:1000][:][:]\n",
    "X_2 = X_train[1000:2000][:][:]\n",
    "Y_2 = Y_train[1000:2000][:][:]\n",
    "X_3 = X_train[2000:3000][:][:]\n",
    "Y_3 = Y_train[2000:3000][:][:]\n",
    "X_4 = X_train[3000:4000][:][:]\n",
    "Y_4 = Y_train[3000:4000][:][:]\n",
    "X_5 = X_train[4000:5000][:][:]\n",
    "Y_5 = Y_train[4000:5000][:][:]\n",
    "\n",
    "print(\"X_0\",X_0.shape,\"\\nX_1\",X_1.shape,\"\\nX_2:\",X_2.shape,\"\\nX_3:\",X_3.shape,\"\\nX_4:\",X_4.shape,\"\\nX_5:\",X_5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Fold Cross Validation Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (5000, 7168, 2) \n",
      "Y_train: (5000,) \n",
      "X_test: (1000, 7168, 2) \n",
      "Y_test: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Set Fold from 0 to 5: 0 is no Cross Validation and 1-5 are cross validation folds.\n",
    "fold = 0\n",
    "\n",
    "if fold == 0:\n",
    "    exit()\n",
    "\n",
    "        \n",
    "if fold == 1:\n",
    "    X_test = X_1\n",
    "    Y_test = Y_1\n",
    "    X_train = np.concatenate((X_2,X_3,X_4,X_5),axis=0)\n",
    "    Y_train = np.concatenate((Y_2,Y_3,Y_4,Y_5),axis=0)\n",
    "    \n",
    "if fold == 2:\n",
    "    X_test = X_2\n",
    "    Y_test = Y_2\n",
    "    X_train = np.concatenate((X_1,X_3,X_4,X_5),axis=0)\n",
    "    Y_train = np.concatenate((Y_1,Y_3,Y_4,Y_5),axis=0)\n",
    "if fold == 3:\n",
    "    X_test = X_3\n",
    "    Y_test = Y_3\n",
    "    X_train = np.concatenate((X_1,X_2,X_4,X_5),axis=0)\n",
    "    Y_train = np.concatenate((Y_1,Y_2,Y_4,Y_5),axis=0)\n",
    "if fold == 4:\n",
    "    X_test = X_4\n",
    "    Y_test = Y_4\n",
    "    X_train = np.concatenate((X_1,X_2,X_3,X_5),axis=0)\n",
    "    Y_train = np.concatenate((Y_1,Y_2,Y_3,Y_5),axis=0)\n",
    "if fold == 5:\n",
    "    X_test = X_5\n",
    "    Y_test = Y_5\n",
    "    X_train = np.concatenate((X_1,X_2,X_3,X_4),axis=0)\n",
    "    Y_train = np.concatenate((Y_1,Y_2,Y_3,Y_4),axis=0)        \n",
    "\n",
    "# Number of Output Categories\n",
    "n_classes = 6\n",
    "\n",
    "# Print Train and Test Split Shapes\n",
    "print(\"X_train:\",X_train.shape,\"\\nY_train:\",Y_train.shape,\"\\nX_test:\",X_test.shape,\"\\nY_test:\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a LSTM layer with 1 output, and ambiguous input data length.\n",
    "model.add(layers.LSTM(256,input_shape=(sample_size,2),return_sequences=True,kernel_regularizer='l2'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5)) #dropout with 50% rate\n",
    "model.add(layers.LSTM(128,return_sequences=True,kernel_regularizer='l2'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Bidirectional(layers.GRU(64,return_sequences=True,kernel_regularizer='l2')))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Bidirectional(layers.GRU(32,return_sequences=False,kernel_regularizer='l2')))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(16,activation='relu',kernel_regularizer='l2'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(n_classes, activation='softmax',kernel_regularizer='l2'))\n",
    "\n",
    "# Compile Model\n",
    "optimizer = Adam(learning_rate=2*1e-5)\n",
    "history = model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Training Parameters and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Parameters\n",
    "num_epochs = 8\n",
    "num_batch_size = 200\n",
    "\n",
    "# Save the most accurate model to file. (Verbosity Gives more information)\n",
    "checkpointer = ModelCheckpoint(filepath=\"SavedModels/checkpointModel.hdf5\", verbose=1,save_best_only=True)\n",
    "\n",
    "# Start the timer\n",
    "start = datetime.now()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,Y_train,batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test,Y_test), callbacks=[checkpointer],verbose=1)\n",
    "\n",
    "# Get and Print Model Validation Accuracy\n",
    "test_accuracy=model.evaluate(X_test,Y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Saved Model to C Code using tflite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('SavedModels/checkpointModel.hdf5')\n",
    "converter = lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.experimental_new_converter=True\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "tfmodel = converter.convert()\n",
    "open('new_model.tflite', 'wb').write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv_viz2",
   "language": "python",
   "name": "tfenv_viz2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
